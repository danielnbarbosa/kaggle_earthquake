{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHANGES\n",
    "\n",
    "- trying L2 loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show versions\n",
    "print('fastai:', fastai.__version__)\n",
    "print('pytorch:', torch.__version__)\n",
    "print('python:', sys.version.split(' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'v10'       # this should match the notebook filename\n",
    "\n",
    "seed = 42\n",
    "arch = models.resnet50\n",
    "size = 392\n",
    "bs = 16\n",
    "num_workers = 6     # set to available cores\n",
    "\n",
    "scale = 1           # number to divide y by to help normalize values\n",
    "transform = 'spectogram'  # which time series to visual transformation to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumes the following has been generated using `gen_spectogram`:\n",
    "- `X_train.csv`\n",
    "- `y_train.csv`\n",
    "- `train_images`\n",
    "- `test_images`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths\n",
    "path = pathlib.Path.home()/'.fastai/data/kaggle_earthquake'\n",
    "img_path = path/f'train_images/{transform}'\n",
    "save_path = path/'saved_models'\n",
    "os.makedirs(save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load segment CSVs\n",
    "#pd.options.display.precision = 15\n",
    "X_train = pd.read_csv(path/'X_train.csv', index_col=0)\n",
    "y_train = pd.read_csv(path/'y_train.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph y_train\n",
    "print('min:', min(y_train['time_to_failure']))\n",
    "print('max:', max(y_train['time_to_failure']))\n",
    "plt.figure()\n",
    "plt.plot(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale down the labels\n",
    "def gen_label(path):\n",
    "    id = int(path.name.split('_')[1].split('.')[0])\n",
    "    ttf = y_train.iloc[id]['time_to_failure']\n",
    "    return ttf / scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_label(img_path/'seg_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no data augmentation\n",
    "tfms = get_transforms(do_flip=False, p_affine=0., p_lighting=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_label = 0.\n",
    "#valid_idx = range(3000, len(y_train))\n",
    "\n",
    "src = (ImageList.from_folder(img_path)\n",
    "        .split_by_rand_pct(valid_pct=0.20)\n",
    "        .label_from_func(gen_label, label_cls=FloatList)\n",
    "        .add_test_folder(f'../../test_images/{transform}', label=fake_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (src.transform(tfms, resize_method=ResizeMethod.SQUISH, size=size)\n",
    "        .databunch(bs=bs, num_workers=num_workers)\n",
    "        .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify datasets loaded properly\n",
    "n_train_items = len(data.train_ds)\n",
    "n_valid_items = len(data.valid_ds)\n",
    "n_test_items = len(data.test_ds)\n",
    "\n",
    "print('train: ', n_train_items)\n",
    "print('valid: ', n_valid_items)\n",
    "print('test:  ', n_test_items)\n",
    "print('')\n",
    "print('TOTAL: ', n_train_items + n_valid_items + n_test_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify images and labels match up\n",
    "data.show_batch(4, figsize=(9, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metrics\n",
    "def mae_scaled(preds, targs):\n",
    "    return mean_absolute_error(preds.view(-1)*scale, targs.view(-1)*scale)\n",
    "\n",
    "def mse_scaled(preds, targs):\n",
    "    return mean_squared_error(preds.view(-1)*scale, targs.view(-1)*scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 loss is sum of the all the absolute differences\n",
    "# more robust to outliers\n",
    "l1loss = nn.L1Loss()\n",
    "def l1_loss(preds, targs):\n",
    "    return l1loss(preds.view(-1), targs.view(-1))\n",
    "\n",
    "# L2 loss is sum of the all the squared differences\n",
    "# less robust to outliers\n",
    "l2loss = nn.MSELoss()\n",
    "def l2_loss(preds, targs):\n",
    "    return l2loss(preds.view(-1), targs.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return correct size of fully connected layer based on pre-trained model\n",
    "def final_conv_layer_size(arch):\n",
    "    if arch == models.resnet18 and size == (217, 223): return 512*14*21\n",
    "    elif arch == models.resnet34 and size == (217, 223): return 512*7*7  \n",
    "    elif arch == models.resnet50 and size == (217, 223): return 2048*7*7\n",
    "    elif arch == models.resnet50 and size == 224: return 2048*7*7\n",
    "    elif arch == models.resnet50 and size == 392: return 2048*13*13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom heads\n",
    "small_head = nn.Sequential(Flatten(), nn.Linear(final_conv_layer_size(arch), 1))\n",
    "\n",
    "small_head_sigmoid = nn.Sequential(Flatten(), nn.Linear(final_conv_layer_size(arch), 1),  nn.Sigmoid())\n",
    "\n",
    "medium_head = nn.Sequential(\n",
    "  nn.AvgPool2d(13, 13),\n",
    "  Flatten(), \n",
    "  nn.BatchNorm1d(2048),\n",
    "  nn.Dropout(0.5),\n",
    "  nn.Linear(2048, 256),\n",
    "  nn.ReLU(),\n",
    "  nn.BatchNorm1d(256),\n",
    "  nn.Dropout(0.5),\n",
    "  nn.Linear(256, 1))\n",
    "\n",
    "big_head = nn.Sequential(\n",
    "  nn.AvgPool2d(13, 13),\n",
    "  Flatten(), \n",
    "  nn.BatchNorm1d(2048),\n",
    "  nn.Dropout(0.5),\n",
    "  nn.Linear(2048, 512),\n",
    "  nn.ReLU(),\n",
    "  nn.BatchNorm1d(512),\n",
    "  nn.Dropout(0.5),\n",
    "  nn.Linear(512, 128),\n",
    "  nn.ReLU(),\n",
    "  nn.BatchNorm1d(128),\n",
    "  nn.Dropout(0.5),\n",
    "  nn.Linear(128, 1))\n",
    "\n",
    "\n",
    "big_head_sigmoid = nn.Sequential(\n",
    "  nn.AvgPool2d(13, 13),\n",
    "  Flatten(), \n",
    "  nn.BatchNorm1d(2048),\n",
    "  nn.Dropout(0.5),\n",
    "  nn.Linear(2048, 512),\n",
    "  nn.ReLU(),\n",
    "  nn.BatchNorm1d(512),\n",
    "  nn.Dropout(0.5),\n",
    "  nn.Linear(512, 128),\n",
    "  nn.ReLU(),\n",
    "  nn.BatchNorm1d(128),\n",
    "  nn.Dropout(0.5),\n",
    "  nn.Linear(128, 1),\n",
    "  nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create learner\n",
    "learn = cnn_learner(data, arch,\n",
    "                    custom_head=big_head,\n",
    "                    loss_func=l1_loss,\n",
    "                    metrics=[mean_squared_error, mean_absolute_error, mse_scaled, mae_scaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(learn.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lr_find(learn):\n",
    "    learn.lr_find()\n",
    "    learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_one_cycle(learn, stage, n_epochs, max_lr):\n",
    "    learn.fit_one_cycle(n_epochs, max_lr=max_lr, callbacks=[\n",
    "        SaveModelCallback(learn,\n",
    "                          monitor='mean_absolute_error',\n",
    "                          mode='min',\n",
    "                          every='improvement',\n",
    "                          name=save_path/f'{version}-{stage}-best')])\n",
    "    learn.recorder.plot_losses()\n",
    "    learn.recorder.plot_lr(show_moms=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lr_find(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage = 's1.1'\n",
    "n_epochs = 6\n",
    "max_lr = slice(1.1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_one_cycle(learn, stage, n_epochs, max_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(save_path/f'{version}-s1.1-best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lr_find(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage = 's1.2'\n",
    "n_epochs = 12\n",
    "max_lr = slice(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_one_cycle(learn, stage, n_epochs, max_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(save_path/f'{version}-s1.2-best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find(start_lr=1e-8)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage = 's2.1'\n",
    "n_epochs = 6\n",
    "max_lr = slice(4e-7, 4e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_one_cycle(learn, stage, n_epochs, max_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(save_path/f'{version}-s2.1-best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find(start_lr=1e-8)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage = 's2.2'\n",
    "n_epochs = 24\n",
    "max_lr = slice(2e-7, 2e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_one_cycle(learn, stage, n_epochs, max_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(save_path/f'{version}-s2.2-best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histograms of results\n",
    "def plot_results(dataset):\n",
    "    preds, targs = learn.get_preds(ds_type=dataset)\n",
    "    preds = preds * scale\n",
    "    targs = targs * scale\n",
    "    print('min/max pred: ', min(preds).item(), max(preds).item())\n",
    "    print('min/max targ: ', min(targs).item(), max(targs).item())\n",
    "    fig, ([ax1, ax2], [ax3, ax4]) = plt.subplots(nrows=2, ncols=2, figsize=(18, 12))\n",
    "    ax1.hist(preds.squeeze(), bins=50); ax1.set_xlabel('TTF'); ax1.set_ylabel('preds')\n",
    "    ax2.plot(preds); ax2.set_xlabel('segment')\n",
    "    ax3.hist(targs, bins=50); ax3.set_xlabel('TTF'); ax3.set_ylabel('targs')\n",
    "    ax4.plot(targs); ax4.set_xlabel('segment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(DatasetType.Fix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(DatasetType.Valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(DatasetType.Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds, _ = learn.get_preds(ds_type=DatasetType.Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sample submission\n",
    "submission = pd.read_csv(path/'sample_submission.csv', index_col='seg_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume order of test set is unchanged\n",
    "submission['time_to_failure'] = [test_preds[i].item() * scale for i in range(len(test_preds))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file = path/f'{version}-submission.csv'\n",
    "submission.to_csv(submission_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit to leaderboard\n",
    "! cd $path; kaggle competitions submit -c LANL-Earthquake-Prediction -f $submission_file -m \"Message\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mae_scaled   : 2.157745    \n",
    "# Public Score : 1.689"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
